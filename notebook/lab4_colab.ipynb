{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MzzS33uq5HDZ",
        "outputId": "2867e29b-9678-4f34-c8b3-88e2e174a00a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataTransformerRegistry.enable('vegafusion')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# all the necessary imports\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import altair as alt\n",
        "import gdown\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "alt.data_transformers.enable(\"vegafusion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j41ee1C85HDa",
        "outputId": "1fb6e43d-ec57-4c2f-8103-5b652acb671e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# set the seed\n",
        "manual_seed = 572\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C25f1KLR5HDb"
      },
      "source": [
        "You can adap these two functions for your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QogjyNOT5HDb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train(loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    # iterate throught the data loader\n",
        "    num_sample = 0\n",
        "    for batch in loader:\n",
        "        # load the current batch\n",
        "        batch_input = batch.review\n",
        "        batch_output = batch.label\n",
        "\n",
        "        batch_input = batch_input.to(device)\n",
        "        batch_output = batch_output.to(device)\n",
        "        # forward propagation\n",
        "        # pass the data through the model\n",
        "        model_outputs = model(batch_input)\n",
        "        # compute the loss\n",
        "        cur_loss = criterion(model_outputs, batch_output)\n",
        "        total_loss += cur_loss.cpu().item()\n",
        "\n",
        "        # backward propagation (compute the gradients and update the model)\n",
        "        # clear the buffer\n",
        "        optimizer.zero_grad()\n",
        "        # compute the gradients\n",
        "        cur_loss.backward()\n",
        "        # update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        num_sample += batch_output.shape[0]\n",
        "\n",
        "    return total_loss/num_sample\n",
        "\n",
        "# evaluation logic based on classification accuracy\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
        "        for batch in loader:\n",
        "             # load the current batch\n",
        "            batch_input = batch.review\n",
        "            batch_output = batch.label\n",
        "\n",
        "            batch_input = batch_input.to(device)\n",
        "            # forward propagation\n",
        "            # pass the data through the model\n",
        "            model_outputs = model(batch_input)\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(batch_output.cpu())\n",
        "\n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro')\n",
        "    return accuracy,f1score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c36QlzFe5HDb"
      },
      "outputs": [],
      "source": [
        "# funtion for save prediction\n",
        "def out_prediction(first_name, last_name, prediction_list):\n",
        "    \"\"\"\n",
        "    out_prediction takes three input varibles: first_name, last_name, prediction_list\n",
        "    <first_name>, string, your first name, e.g., Tom\n",
        "    <last_name>, string, your last name, e.g., Smith\n",
        "    <prediction_list>, list of string which includes all your predications of TEST samples\n",
        "                        e.g., ['1star','5star','3star']\n",
        "\n",
        "    Generate a file is named with <yourfirstname>_<yourlastname>_PRED.txt in current directory\n",
        "    \"\"\"\n",
        "    output_file = open(\"{}_{}_PRED.txt\".format(first_name,last_name),'w')\n",
        "    for item in prediction_list:\n",
        "        output_file.write(item+\"\\n\")\n",
        "    output_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgcFa0An5HDc"
      },
      "source": [
        "# Please write code to develop you system. More details are in `Lab4.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZL5FYg7G5HDc",
        "outputId": "a01e5e49-6c90-4858-c82f-0ecc98367bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder 1miNtgEGuJ6F-2cGJ5crifLQUbWnb-wJl yelp_review\n",
            "Retrieving folder 10gkkUq4rNU1HWSUr6GOQ0iwprA5gaV6e .ipynb_checkpoints\n",
            "Processing file 1FrYOgXiu-pVX_TQ57pSUtpwHeu1R4J0k EXAMPLE_GOLD-checkpoint.txt\n",
            "Processing file 1oCDq8R-xBdwcJBr_2KLpzg6WVgQN_Re9 EXAMPLE_PRED_result-checkpoint.txt\n",
            "Processing file 1MpSkPKP6T043NDhpS4zxi4JNgnzB_-Zg EXAMPLE_GOLD.txt\n",
            "Processing file 1ztc0m5FuX1CaTm72iVIZpP_eRxQpl0Wt EXAMPLE_PRED_result.txt\n",
            "Processing file 1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r Scorer.py\n",
            "Processing file 1crLuR8fhJ89RKEVut-UjgIyOz6-y5eh4 test.tsv\n",
            "Processing file 10qVnIg3rvEXlcEA9hynHTKoYDURLQN4v train.tsv\n",
            "Processing file 1zozCGPs0XiJAPl1Il5cSWl9p0Aqv8YgO val.tsv\n",
            "Processing file 1Vr4eJVM9kIK38ZONzamuIrQKf_gi3pw- .Rhistory\n",
            "Processing file 19QypIq9BjPB_WhGIsT8Zrr658VV6uQHC readme.md\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FrYOgXiu-pVX_TQ57pSUtpwHeu1R4J0k\n",
            "To: /content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_GOLD-checkpoint.txt\n",
            "100%|██████████| 21.0k/21.0k [00:00<00:00, 29.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oCDq8R-xBdwcJBr_2KLpzg6WVgQN_Re9\n",
            "To: /content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_PRED_result-checkpoint.txt\n",
            "100%|██████████| 149/149 [00:00<00:00, 531kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MpSkPKP6T043NDhpS4zxi4JNgnzB_-Zg\n",
            "To: /content/data/yelp_review/EXAMPLE_GOLD.txt\n",
            "100%|██████████| 21.0k/21.0k [00:00<00:00, 41.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ztc0m5FuX1CaTm72iVIZpP_eRxQpl0Wt\n",
            "To: /content/data/yelp_review/EXAMPLE_PRED_result.txt\n",
            "100%|██████████| 149/149 [00:00<00:00, 638kB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r\n",
            "From (redirected): https://drive.google.com/uc?id=1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r&confirm=t&uuid=d1b9cde2-621d-49fa-b05b-920687536023\n",
            "To: /content/data/yelp_review/Scorer.py\n",
            "100%|██████████| 3.35k/3.35k [00:00<00:00, 10.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1crLuR8fhJ89RKEVut-UjgIyOz6-y5eh4\n",
            "To: /content/data/yelp_review/test.tsv\n",
            "100%|██████████| 2.63M/2.63M [00:00<00:00, 13.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10qVnIg3rvEXlcEA9hynHTKoYDURLQN4v\n",
            "To: /content/data/yelp_review/train.tsv\n",
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 68.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zozCGPs0XiJAPl1Il5cSWl9p0Aqv8YgO\n",
            "To: /content/data/yelp_review/val.tsv\n",
            "100%|██████████| 2.67M/2.67M [00:00<00:00, 20.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vr4eJVM9kIK38ZONzamuIrQKf_gi3pw-\n",
            "To: /content/data/.Rhistory\n",
            "0.00B [00:00, ?B/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19QypIq9BjPB_WhGIsT8Zrr658VV6uQHC\n",
            "To: /content/data/readme.md\n",
            "100%|██████████| 98.0/98.0 [00:00<00:00, 418kB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_GOLD-checkpoint.txt',\n",
              " '/content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_PRED_result-checkpoint.txt',\n",
              " '/content/data/yelp_review/EXAMPLE_GOLD.txt',\n",
              " '/content/data/yelp_review/EXAMPLE_PRED_result.txt',\n",
              " '/content/data/yelp_review/Scorer.py',\n",
              " '/content/data/yelp_review/test.tsv',\n",
              " '/content/data/yelp_review/train.tsv',\n",
              " '/content/data/yelp_review/val.tsv',\n",
              " '/content/data/.Rhistory',\n",
              " '/content/data/readme.md']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Data download to colab working directory\n",
        "url = f'https://drive.google.com/drive/folders/1zF5s4KRMxpr3OvUY8o_d0xv_YlaZSCsj?usp=drive_link'\n",
        "gdown.download_folder(url, quiet = False, use_cookies = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yAN79H5I5HDc",
        "outputId": "741eb492-b394-4bb2-9ac8-000e14afe8ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content rating\n",
              "0  There are some restaurants that you don't want...  4star\n",
              "1  Lucky for us there was no wait unlike other ti...  4star\n",
              "2  Worst ever Michelin restaurant I have ever bee...  1star\n",
              "3  Came here today to celebrate my birthdays with...  4star\n",
              "4  This, is where hipsters go to get Caribbean fo...  2star"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce362c25-36a6-404d-b7f3-c20428b22347\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>There are some restaurants that you don't want...</td>\n",
              "      <td>4star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lucky for us there was no wait unlike other ti...</td>\n",
              "      <td>4star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Worst ever Michelin restaurant I have ever bee...</td>\n",
              "      <td>1star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Came here today to celebrate my birthdays with...</td>\n",
              "      <td>4star</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This, is where hipsters go to get Caribbean fo...</td>\n",
              "      <td>2star</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce362c25-36a6-404d-b7f3-c20428b22347')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce362c25-36a6-404d-b7f3-c20428b22347 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce362c25-36a6-404d-b7f3-c20428b22347');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_set",
              "summary": "{\n  \"name\": \"train_set\",\n  \"rows\": 28000,\n  \"fields\": [\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27998,\n        \"samples\": [\n          \"This restaurant has been on my list of ones to try in the Downtown Brooklyn area just because it looked so nice as I drove by in the past. The decor is definitely super modern, inviting and even intimate. I liked the layout and unconventional plate setting. The service was attentive and consistent; the manager was out and about mingling with the patrons. The whole vibe was nice then we started eating. In general, I am not a fan of fusion restaurants because one flavor usually is sacrificed for the other. In general that's how I felt. The appetizers were definitely Korean and tasty. The main course was good yet culinarily ambiguous. Their side options were not too varied or appetizing.  The experience did get better when our desserts arrived. The desserts were minimal but tasty. Glad that I was able to try it through NYC Restaurant Week because I don't know if I would paid regular for this.\",\n          \"I wanted to stop for a drink this afternoon; the weather was perfect. \\u00a0But once we sat down briefly, my wife thought that the boat was moving too much - so we went back ashore. In a flat calm it was bouncing a bit.  Mid-afternoon on a Friday the bar was busy. \\u00a0Plenty of empty tables in the restaurant section.\",\n          \"Such a fun, busy, vibrant place. Perfect for a hot summer day/night hangout. They have plenty of space both indoors and outdoors. Colorful decor and loud music makes, and most importantly, great frozen drinks make you feel like you're on a mini vacation.  Atmosphere: very casual and fun. Good for big groups.  Drinks:  Frozen Margarita - fresh, fresh and fresh! Delightfully sour and quite sweet.  Frozen Mojito - minty taste is well infused. I liked it more than margarita.  Food:  Grilled corn - it came out like warm, but the flavor was there. Not that impressive.  Chicken and Waffle on Stick - so good!! Well seasoned chicken wrapped in crispy waffle, drizzled with maple syrup. I could have six of these!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"1star\",\n          \"3star\",\n          \"2star\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "### Import for EDA\n",
        "train_set = pd.read_csv('data/yelp_review/train.tsv', sep = '\\t')\n",
        "dev_set = pd.read_csv('data/yelp_review/val.tsv', sep = '\\t')\n",
        "test_set = pd.read_csv('data/yelp_review/test.tsv', sep = '\\t')\n",
        "\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oed1Gh3A5HDc"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "CuVSiPiI5HDc",
        "outputId": "50c876cd-b7a1-4797-ef62-6ad940a3cd35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442.vega-embed details,\n",
              "  #altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-9bdd20ab8e9c486e9cbffcfb73e23442\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"$schema\": \"https://vega.github.io/schema/vega/v5.json\", \"data\": [{\"name\": \"source_0\"}, {\"name\": \"data_0\"}, {\"name\": \"data_1\", \"values\": [{\"__count\": 5531, \"rating\": \"4star\"}, {\"__count\": 5619, \"rating\": \"1star\"}, {\"__count\": 5616, \"rating\": \"2star\"}, {\"__count\": 5651, \"rating\": \"5star\"}, {\"__count\": 5583, \"rating\": \"3star\"}]}, {\"name\": \"data_2\", \"values\": [{\"__count\": 400, \"__count_end\": 702.0, \"__count_start\": 302.0, \"bin_maxbins_60_review_length\": 140.0, \"bin_maxbins_60_review_length_end\": 160.0, \"rating\": \"4star\"}, {\"__count\": 602, \"__count_end\": 1276.0, \"__count_start\": 674.0, \"bin_maxbins_60_review_length\": 60.0, \"bin_maxbins_60_review_length_end\": 80.0, \"rating\": \"4star\"}, {\"__count\": 674, \"__count_end\": 2999.0, \"__count_start\": 2325.0, \"bin_maxbins_60_review_length\": 20.0, \"bin_maxbins_60_review_length_end\": 40.0, \"rating\": \"1star\"}, {\"__count\": 211, \"__count_end\": 352.0, \"__count_start\": 141.0, \"bin_maxbins_60_review_length\": 220.0, \"bin_maxbins_60_review_length_end\": 240.0, \"rating\": \"4star\"}, {\"__count\": 228, \"__count_end\": 819.0, \"__count_start\": 591.0, \"bin_maxbins_60_review_length\": 200.0, \"bin_maxbins_60_review_length_end\": 220.0, \"rating\": \"2star\"}, {\"__count\": 465, \"__count_end\": 465.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 100.0, \"bin_maxbins_60_review_length_end\": 120.0, \"rating\": \"5star\"}, {\"__count\": 261, \"__count_end\": 1471.0, \"__count_start\": 1210.0, \"bin_maxbins_60_review_length\": 160.0, \"bin_maxbins_60_review_length_end\": 180.0, \"rating\": \"1star\"}, {\"__count\": 610, \"__count_end\": 1697.0, \"__count_start\": 1087.0, \"bin_maxbins_60_review_length\": 80.0, \"bin_maxbins_60_review_length_end\": 100.0, \"rating\": \"3star\"}, {\"__count\": 396, \"__count_end\": 2112.0, \"__count_start\": 1716.0, \"bin_maxbins_60_review_length\": 120.0, \"bin_maxbins_60_review_length_end\": 140.0, \"rating\": \"1star\"}, {\"__count\": 457, \"__count_end\": 1881.0, \"__count_start\": 1424.0, \"bin_maxbins_60_review_length\": 20.0, \"bin_maxbins_60_review_length_end\": 40.0, \"rating\": \"3star\"}, {\"__count\": 433, \"__count_end\": 1716.0, \"__count_start\": 1283.0, \"bin_maxbins_60_review_length\": 120.0, \"bin_maxbins_60_review_length_end\": 140.0, \"rating\": \"2star\"}, {\"__count\": 395, \"__count_end\": 1489.0, \"__count_start\": 1094.0, \"bin_maxbins_60_review_length\": 140.0, \"bin_maxbins_60_review_length_end\": 160.0, \"rating\": \"2star\"}, {\"__count\": 240, \"__count_end\": 240.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 0.0, \"bin_maxbins_60_review_length_end\": 20.0, \"rating\": \"5star\"}, {\"__count\": 34, \"__count_end\": 62.0, \"__count_start\": 28.0, \"bin_maxbins_60_review_length\": 400.0, \"bin_maxbins_60_review_length_end\": 420.0, \"rating\": \"4star\"}, {\"__count\": 302, \"__count_end\": 302.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 140.0, \"bin_maxbins_60_review_length_end\": 160.0, \"rating\": \"5star\"}, {\"__count\": 153, \"__count_end\": 541.0, \"__count_start\": 388.0, \"bin_maxbins_60_review_length\": 240.0, \"bin_maxbins_60_review_length_end\": 260.0, \"rating\": \"2star\"}, {\"__count\": 821, \"__count_end\": 821.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 40.0, \"bin_maxbins_60_review_length_end\": 60.0, \"rating\": \"5star\"}, {\"__count\": 869, \"__count_end\": 869.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 20.0, \"bin_maxbins_60_review_length_end\": 40.0, \"rating\": \"5star\"}, {\"__count\": 7, \"__count_end\": 9.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 580.0, \"bin_maxbins_60_review_length_end\": 600.0, \"rating\": \"4star\"}, {\"__count\": 25, \"__count_end\": 55.0, \"__count_start\": 30.0, \"bin_maxbins_60_review_length\": 500.0, \"bin_maxbins_60_review_length_end\": 520.0, \"rating\": \"2star\"}, {\"__count\": 592, \"__count_end\": 3126.0, \"__count_start\": 2534.0, \"bin_maxbins_60_review_length\": 60.0, \"bin_maxbins_60_review_length_end\": 80.0, \"rating\": \"1star\"}, {\"__count\": 340, \"__count_end\": 931.0, \"__count_start\": 591.0, \"bin_maxbins_60_review_length\": 160.0, \"bin_maxbins_60_review_length_end\": 180.0, \"rating\": \"3star\"}, {\"__count\": 543, \"__count_end\": 543.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 80.0, \"bin_maxbins_60_review_length_end\": 100.0, \"rating\": \"5star\"}, {\"__count\": 518, \"__count_end\": 1462.0, \"__count_start\": 944.0, \"bin_maxbins_60_review_length\": 100.0, \"bin_maxbins_60_review_length_end\": 120.0, \"rating\": \"3star\"}, {\"__count\": 552, \"__count_end\": 1988.0, \"__count_start\": 1436.0, \"bin_maxbins_60_review_length\": 40.0, \"bin_maxbins_60_review_length_end\": 60.0, \"rating\": \"3star\"}, {\"__count\": 90, \"__count_end\": 512.0, \"__count_start\": 422.0, \"bin_maxbins_60_review_length\": 0.0, \"bin_maxbins_60_review_length_end\": 20.0, \"rating\": \"2star\"}, {\"__count\": 182, \"__count_end\": 1001.0, \"__count_start\": 819.0, \"bin_maxbins_60_review_length\": 200.0, \"bin_maxbins_60_review_length_end\": 220.0, \"rating\": \"1star\"}, {\"__count\": 615, \"__count_end\": 1436.0, \"__count_start\": 821.0, \"bin_maxbins_60_review_length\": 40.0, \"bin_maxbins_60_review_length_end\": 60.0, \"rating\": \"4star\"}, {\"__count\": 381, \"__count_end\": 1870.0, \"__count_start\": 1489.0, \"bin_maxbins_60_review_length\": 140.0, \"bin_maxbins_60_review_length_end\": 160.0, \"rating\": \"1star\"}, {\"__count\": 535, \"__count_end\": 2823.0, \"__count_start\": 2288.0, \"bin_maxbins_60_review_length\": 80.0, \"bin_maxbins_60_review_length_end\": 100.0, \"rating\": \"1star\"}, {\"__count\": 111, \"__count_end\": 192.0, \"__count_start\": 81.0, \"bin_maxbins_60_review_length\": 260.0, \"bin_maxbins_60_review_length_end\": 280.0, \"rating\": \"4star\"}, {\"__count\": 555, \"__count_end\": 1424.0, \"__count_start\": 869.0, \"bin_maxbins_60_review_length\": 20.0, \"bin_maxbins_60_review_length_end\": 40.0, \"rating\": \"4star\"}, {\"__count\": 667, \"__count_end\": 2534.0, \"__count_start\": 1867.0, \"bin_maxbins_60_review_length\": 60.0, \"bin_maxbins_60_review_length_end\": 80.0, \"rating\": \"2star\"}, {\"__count\": 128, \"__count_end\": 320.0, \"__count_start\": 192.0, \"bin_maxbins_60_review_length\": 260.0, \"bin_maxbins_60_review_length_end\": 280.0, \"rating\": \"3star\"}, {\"__count\": 702, \"__count_end\": 3283.0, \"__count_start\": 2581.0, \"bin_maxbins_60_review_length\": 40.0, \"bin_maxbins_60_review_length_end\": 60.0, \"rating\": \"1star\"}, {\"__count\": 47, \"__count_end\": 47.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 340.0, \"bin_maxbins_60_review_length_end\": 360.0, \"rating\": \"5star\"}, {\"__count\": 591, \"__count_end\": 2288.0, \"__count_start\": 1697.0, \"bin_maxbins_60_review_length\": 80.0, \"bin_maxbins_60_review_length_end\": 100.0, \"rating\": \"2star\"}, {\"__count\": 21, \"__count_end\": 91.0, \"__count_start\": 70.0, \"bin_maxbins_60_review_length\": 460.0, \"bin_maxbins_60_review_length_end\": 480.0, \"rating\": \"1star\"}, {\"__count\": 593, \"__count_end\": 2581.0, \"__count_start\": 1988.0, \"bin_maxbins_60_review_length\": 40.0, \"bin_maxbins_60_review_length_end\": 60.0, \"rating\": \"2star\"}, {\"__count\": 591, \"__count_end\": 1867.0, \"__count_start\": 1276.0, \"bin_maxbins_60_review_length\": 60.0, \"bin_maxbins_60_review_length_end\": 80.0, \"rating\": \"3star\"}, {\"__count\": 497, \"__count_end\": 2498.0, \"__count_start\": 2001.0, \"bin_maxbins_60_review_length\": 100.0, \"bin_maxbins_60_review_length_end\": 120.0, \"rating\": \"1star\"}, {\"__count\": 66, \"__count_end\": 331.0, \"__count_start\": 265.0, \"bin_maxbins_60_review_length\": 320.0, \"bin_maxbins_60_review_length_end\": 340.0, \"rating\": \"1star\"}, {\"__count\": 478, \"__count_end\": 1283.0, \"__count_start\": 805.0, \"bin_maxbins_60_review_length\": 120.0, \"bin_maxbins_60_review_length_end\": 140.0, \"rating\": \"3star\"}, {\"__count\": 48, \"__count_end\": 204.0, \"__count_start\": 156.0, \"bin_maxbins_60_review_length\": 360.0, \"bin_maxbins_60_review_length_end\": 380.0, \"rating\": \"1star\"}, {\"__count\": 674, \"__count_end\": 674.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 60.0, \"bin_maxbins_60_review_length_end\": 80.0, \"rating\": \"5star\"}, {\"__count\": 69, \"__count_end\": 157.0, \"__count_start\": 88.0, \"bin_maxbins_60_review_length\": 340.0, \"bin_maxbins_60_review_length_end\": 360.0, \"rating\": \"3star\"}, {\"__count\": 250, \"__count_end\": 456.0, \"__count_start\": 206.0, \"bin_maxbins_60_review_length\": 180.0, \"bin_maxbins_60_review_length_end\": 200.0, \"rating\": \"4star\"}, {\"__count\": 331, \"__count_end\": 591.0, \"__count_start\": 260.0, \"bin_maxbins_60_review_length\": 160.0, \"bin_maxbins_60_review_length_end\": 180.0, \"rating\": \"4star\"}, {\"__count\": 279, \"__count_end\": 1210.0, \"__count_start\": 931.0, \"bin_maxbins_60_review_length\": 160.0, \"bin_maxbins_60_review_length_end\": 180.0, \"rating\": \"2star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 660.0, \"bin_maxbins_60_review_length_end\": 680.0, \"rating\": \"5star\"}, {\"__count\": 224, \"__count_end\": 591.0, \"__count_start\": 367.0, \"bin_maxbins_60_review_length\": 200.0, \"bin_maxbins_60_review_length_end\": 220.0, \"rating\": \"3star\"}, {\"__count\": 133, \"__count_end\": 388.0, \"__count_start\": 255.0, \"bin_maxbins_60_review_length\": 240.0, \"bin_maxbins_60_review_length_end\": 260.0, \"rating\": \"3star\"}, {\"__count\": 253, \"__count_end\": 974.0, \"__count_start\": 721.0, \"bin_maxbins_60_review_length\": 180.0, \"bin_maxbins_60_review_length_end\": 200.0, \"rating\": \"2star\"}, {\"__count\": 430, \"__count_end\": 805.0, \"__count_start\": 375.0, \"bin_maxbins_60_review_length\": 120.0, \"bin_maxbins_60_review_length_end\": 140.0, \"rating\": \"4star\"}, {\"__count\": 539, \"__count_end\": 2001.0, \"__count_start\": 1462.0, \"bin_maxbins_60_review_length\": 100.0, \"bin_maxbins_60_review_length_end\": 120.0, \"rating\": \"2star\"}, {\"__count\": 479, \"__count_end\": 944.0, \"__count_start\": 465.0, \"bin_maxbins_60_review_length\": 100.0, \"bin_maxbins_60_review_length_end\": 120.0, \"rating\": \"4star\"}, {\"__count\": 140, \"__count_end\": 890.0, \"__count_start\": 750.0, \"bin_maxbins_60_review_length\": 220.0, \"bin_maxbins_60_review_length_end\": 240.0, \"rating\": \"1star\"}, {\"__count\": 99, \"__count_end\": 99.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 240.0, \"bin_maxbins_60_review_length_end\": 260.0, \"rating\": \"5star\"}, {\"__count\": 54, \"__count_end\": 211.0, \"__count_start\": 157.0, \"bin_maxbins_60_review_length\": 340.0, \"bin_maxbins_60_review_length_end\": 360.0, \"rating\": \"2star\"}, {\"__count\": 544, \"__count_end\": 1087.0, \"__count_start\": 543.0, \"bin_maxbins_60_review_length\": 80.0, \"bin_maxbins_60_review_length_end\": 100.0, \"rating\": \"4star\"}, {\"__count\": 392, \"__count_end\": 1094.0, \"__count_start\": 702.0, \"bin_maxbins_60_review_length\": 140.0, \"bin_maxbins_60_review_length_end\": 160.0, \"rating\": \"3star\"}, {\"__count\": 17, \"__count_end\": 41.0, \"__count_start\": 24.0, \"bin_maxbins_60_review_length\": 440.0, \"bin_maxbins_60_review_length_end\": 460.0, \"rating\": \"3star\"}, {\"__count\": 265, \"__count_end\": 721.0, \"__count_start\": 456.0, \"bin_maxbins_60_review_length\": 180.0, \"bin_maxbins_60_review_length_end\": 200.0, \"rating\": \"3star\"}, {\"__count\": 444, \"__count_end\": 2325.0, \"__count_start\": 1881.0, \"bin_maxbins_60_review_length\": 20.0, \"bin_maxbins_60_review_length_end\": 40.0, \"rating\": \"2star\"}, {\"__count\": 31, \"__count_end\": 119.0, \"__count_start\": 88.0, \"bin_maxbins_60_review_length\": 420.0, \"bin_maxbins_60_review_length_end\": 440.0, \"rating\": \"1star\"}, {\"__count\": 18, \"__count_end\": 58.0, \"__count_start\": 40.0, \"bin_maxbins_60_review_length\": 480.0, \"bin_maxbins_60_review_length_end\": 500.0, \"rating\": \"2star\"}, {\"__count\": 375, \"__count_end\": 375.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 120.0, \"bin_maxbins_60_review_length_end\": 140.0, \"rating\": \"5star\"}, {\"__count\": 216, \"__count_end\": 367.0, \"__count_start\": 151.0, \"bin_maxbins_60_review_length\": 200.0, \"bin_maxbins_60_review_length_end\": 220.0, \"rating\": \"4star\"}, {\"__count\": 260, \"__count_end\": 260.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 160.0, \"bin_maxbins_60_review_length_end\": 180.0, \"rating\": \"5star\"}, {\"__count\": 206, \"__count_end\": 206.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 180.0, \"bin_maxbins_60_review_length_end\": 200.0, \"rating\": \"5star\"}, {\"__count\": 220, \"__count_end\": 1194.0, \"__count_start\": 974.0, \"bin_maxbins_60_review_length\": 180.0, \"bin_maxbins_60_review_length_end\": 200.0, \"rating\": \"1star\"}, {\"__count\": 214, \"__count_end\": 566.0, \"__count_start\": 352.0, \"bin_maxbins_60_review_length\": 220.0, \"bin_maxbins_60_review_length_end\": 240.0, \"rating\": \"3star\"}, {\"__count\": 12, \"__count_end\": 12.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 420.0, \"bin_maxbins_60_review_length_end\": 440.0, \"rating\": \"5star\"}, {\"__count\": 41, \"__count_end\": 88.0, \"__count_start\": 47.0, \"bin_maxbins_60_review_length\": 340.0, \"bin_maxbins_60_review_length_end\": 360.0, \"rating\": \"4star\"}, {\"__count\": 184, \"__count_end\": 750.0, \"__count_start\": 566.0, \"bin_maxbins_60_review_length\": 220.0, \"bin_maxbins_60_review_length_end\": 240.0, \"rating\": \"2star\"}, {\"__count\": 141, \"__count_end\": 141.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 220.0, \"bin_maxbins_60_review_length_end\": 240.0, \"rating\": \"5star\"}, {\"__count\": 75, \"__count_end\": 320.0, \"__count_start\": 245.0, \"bin_maxbins_60_review_length\": 300.0, \"bin_maxbins_60_review_length_end\": 320.0, \"rating\": \"2star\"}, {\"__count\": 93, \"__count_end\": 489.0, \"__count_start\": 396.0, \"bin_maxbins_60_review_length\": 280.0, \"bin_maxbins_60_review_length_end\": 300.0, \"rating\": \"1star\"}, {\"__count\": 34, \"__count_end\": 88.0, \"__count_start\": 54.0, \"bin_maxbins_60_review_length\": 420.0, \"bin_maxbins_60_review_length_end\": 440.0, \"rating\": \"2star\"}, {\"__count\": 156, \"__count_end\": 255.0, \"__count_start\": 99.0, \"bin_maxbins_60_review_length\": 240.0, \"bin_maxbins_60_review_length_end\": 260.0, \"rating\": \"4star\"}, {\"__count\": 39, \"__count_end\": 175.0, \"__count_start\": 136.0, \"bin_maxbins_60_review_length\": 380.0, \"bin_maxbins_60_review_length_end\": 400.0, \"rating\": \"1star\"}, {\"__count\": 85, \"__count_end\": 265.0, \"__count_start\": 180.0, \"bin_maxbins_60_review_length\": 320.0, \"bin_maxbins_60_review_length_end\": 340.0, \"rating\": \"2star\"}, {\"__count\": 46, \"__count_end\": 156.0, \"__count_start\": 110.0, \"bin_maxbins_60_review_length\": 360.0, \"bin_maxbins_60_review_length_end\": 380.0, \"rating\": \"2star\"}, {\"__count\": 3, \"__count_end\": 8.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 680.0, \"bin_maxbins_60_review_length_end\": 700.0, \"rating\": \"3star\"}, {\"__count\": 12, \"__count_end\": 24.0, \"__count_start\": 12.0, \"bin_maxbins_60_review_length\": 520.0, \"bin_maxbins_60_review_length_end\": 540.0, \"rating\": \"3star\"}, {\"__count\": 94, \"__count_end\": 396.0, \"__count_start\": 302.0, \"bin_maxbins_60_review_length\": 280.0, \"bin_maxbins_60_review_length_end\": 300.0, \"rating\": \"2star\"}, {\"__count\": 16, \"__count_end\": 28.0, \"__count_start\": 12.0, \"bin_maxbins_60_review_length\": 420.0, \"bin_maxbins_60_review_length_end\": 440.0, \"rating\": \"4star\"}, {\"__count\": 148, \"__count_end\": 660.0, \"__count_start\": 512.0, \"bin_maxbins_60_review_length\": 0.0, \"bin_maxbins_60_review_length_end\": 20.0, \"rating\": \"1star\"}, {\"__count\": 97, \"__count_end\": 245.0, \"__count_start\": 148.0, \"bin_maxbins_60_review_length\": 300.0, \"bin_maxbins_60_review_length_end\": 320.0, \"rating\": \"3star\"}, {\"__count\": 38, \"__count_end\": 63.0, \"__count_start\": 25.0, \"bin_maxbins_60_review_length\": 360.0, \"bin_maxbins_60_review_length_end\": 380.0, \"rating\": \"4star\"}, {\"__count\": 73, \"__count_end\": 393.0, \"__count_start\": 320.0, \"bin_maxbins_60_review_length\": 300.0, \"bin_maxbins_60_review_length_end\": 320.0, \"rating\": \"1star\"}, {\"__count\": 7, \"__count_end\": 14.0, \"__count_start\": 7.0, \"bin_maxbins_60_review_length\": 720.0, \"bin_maxbins_60_review_length_end\": 740.0, \"rating\": \"1star\"}, {\"__count\": 78, \"__count_end\": 78.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 280.0, \"bin_maxbins_60_review_length_end\": 300.0, \"rating\": \"5star\"}, {\"__count\": 12, \"__count_end\": 36.0, \"__count_start\": 24.0, \"bin_maxbins_60_review_length\": 540.0, \"bin_maxbins_60_review_length_end\": 560.0, \"rating\": \"1star\"}, {\"__count\": 69, \"__count_end\": 116.0, \"__count_start\": 47.0, \"bin_maxbins_60_review_length\": 320.0, \"bin_maxbins_60_review_length_end\": 340.0, \"rating\": \"4star\"}, {\"__count\": 79, \"__count_end\": 148.0, \"__count_start\": 69.0, \"bin_maxbins_60_review_length\": 300.0, \"bin_maxbins_60_review_length_end\": 320.0, \"rating\": \"4star\"}, {\"__count\": 6, \"__count_end\": 12.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 520.0, \"bin_maxbins_60_review_length_end\": 540.0, \"rating\": \"4star\"}, {\"__count\": 64, \"__count_end\": 180.0, \"__count_start\": 116.0, \"bin_maxbins_60_review_length\": 320.0, \"bin_maxbins_60_review_length_end\": 340.0, \"rating\": \"3star\"}, {\"__count\": 57, \"__count_end\": 268.0, \"__count_start\": 211.0, \"bin_maxbins_60_review_length\": 340.0, \"bin_maxbins_60_review_length_end\": 360.0, \"rating\": \"1star\"}, {\"__count\": 151, \"__count_end\": 151.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 200.0, \"bin_maxbins_60_review_length_end\": 220.0, \"rating\": \"5star\"}, {\"__count\": 105, \"__count_end\": 302.0, \"__count_start\": 197.0, \"bin_maxbins_60_review_length\": 280.0, \"bin_maxbins_60_review_length_end\": 300.0, \"rating\": \"3star\"}, {\"__count\": 23, \"__count_end\": 54.0, \"__count_start\": 31.0, \"bin_maxbins_60_review_length\": 460.0, \"bin_maxbins_60_review_length_end\": 480.0, \"rating\": \"3star\"}, {\"__count\": 16, \"__count_end\": 30.0, \"__count_start\": 14.0, \"bin_maxbins_60_review_length\": 500.0, \"bin_maxbins_60_review_length_end\": 520.0, \"rating\": \"3star\"}, {\"__count\": 16, \"__count_end\": 70.0, \"__count_start\": 54.0, \"bin_maxbins_60_review_length\": 460.0, \"bin_maxbins_60_review_length_end\": 480.0, \"rating\": \"2star\"}, {\"__count\": 119, \"__count_end\": 197.0, \"__count_start\": 78.0, \"bin_maxbins_60_review_length\": 280.0, \"bin_maxbins_60_review_length_end\": 300.0, \"rating\": \"4star\"}, {\"__count\": 11, \"__count_end\": 20.0, \"__count_start\": 9.0, \"bin_maxbins_60_review_length\": 600.0, \"bin_maxbins_60_review_length_end\": 620.0, \"rating\": \"2star\"}, {\"__count\": 126, \"__count_end\": 667.0, \"__count_start\": 541.0, \"bin_maxbins_60_review_length\": 240.0, \"bin_maxbins_60_review_length_end\": 260.0, \"rating\": \"1star\"}, {\"__count\": 14, \"__count_end\": 14.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 460.0, \"bin_maxbins_60_review_length_end\": 480.0, \"rating\": \"5star\"}, {\"__count\": 19, \"__count_end\": 49.0, \"__count_start\": 30.0, \"bin_maxbins_60_review_length\": 560.0, \"bin_maxbins_60_review_length_end\": 580.0, \"rating\": \"1star\"}, {\"__count\": 28, \"__count_end\": 28.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 400.0, \"bin_maxbins_60_review_length_end\": 420.0, \"rating\": \"5star\"}, {\"__count\": 81, \"__count_end\": 81.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 260.0, \"bin_maxbins_60_review_length_end\": 280.0, \"rating\": \"5star\"}, {\"__count\": 94, \"__count_end\": 334.0, \"__count_start\": 240.0, \"bin_maxbins_60_review_length\": 0.0, \"bin_maxbins_60_review_length_end\": 20.0, \"rating\": \"4star\"}, {\"__count\": 47, \"__count_end\": 110.0, \"__count_start\": 63.0, \"bin_maxbins_60_review_length\": 360.0, \"bin_maxbins_60_review_length_end\": 380.0, \"rating\": \"3star\"}, {\"__count\": 19, \"__count_end\": 53.0, \"__count_start\": 34.0, \"bin_maxbins_60_review_length\": 520.0, \"bin_maxbins_60_review_length_end\": 540.0, \"rating\": \"1star\"}, {\"__count\": 136, \"__count_end\": 456.0, \"__count_start\": 320.0, \"bin_maxbins_60_review_length\": 260.0, \"bin_maxbins_60_review_length_end\": 280.0, \"rating\": \"2star\"}, {\"__count\": 47, \"__count_end\": 47.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 320.0, \"bin_maxbins_60_review_length_end\": 340.0, \"rating\": \"5star\"}, {\"__count\": 26, \"__count_end\": 54.0, \"__count_start\": 28.0, \"bin_maxbins_60_review_length\": 420.0, \"bin_maxbins_60_review_length_end\": 440.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 940.0, \"bin_maxbins_60_review_length_end\": 960.0, \"rating\": \"5star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 740.0, \"bin_maxbins_60_review_length_end\": 760.0, \"rating\": \"5star\"}, {\"__count\": 18, \"__count_end\": 76.0, \"__count_start\": 58.0, \"bin_maxbins_60_review_length\": 480.0, \"bin_maxbins_60_review_length_end\": 500.0, \"rating\": \"1star\"}, {\"__count\": 29, \"__count_end\": 91.0, \"__count_start\": 62.0, \"bin_maxbins_60_review_length\": 400.0, \"bin_maxbins_60_review_length_end\": 420.0, \"rating\": \"3star\"}, {\"__count\": 5, \"__count_end\": 20.0, \"__count_start\": 15.0, \"bin_maxbins_60_review_length\": 540.0, \"bin_maxbins_60_review_length_end\": 560.0, \"rating\": \"3star\"}, {\"__count\": 88, \"__count_end\": 422.0, \"__count_start\": 334.0, \"bin_maxbins_60_review_length\": 0.0, \"bin_maxbins_60_review_length_end\": 20.0, \"rating\": \"3star\"}, {\"__count\": 121, \"__count_end\": 577.0, \"__count_start\": 456.0, \"bin_maxbins_60_review_length\": 260.0, \"bin_maxbins_60_review_length_end\": 280.0, \"rating\": \"1star\"}, {\"__count\": 19, \"__count_end\": 74.0, \"__count_start\": 55.0, \"bin_maxbins_60_review_length\": 500.0, \"bin_maxbins_60_review_length_end\": 520.0, \"rating\": \"1star\"}, {\"__count\": 4, \"__count_end\": 4.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 640.0, \"bin_maxbins_60_review_length_end\": 660.0, \"rating\": \"5star\"}, {\"__count\": 49, \"__count_end\": 136.0, \"__count_start\": 87.0, \"bin_maxbins_60_review_length\": 380.0, \"bin_maxbins_60_review_length_end\": 400.0, \"rating\": \"2star\"}, {\"__count\": 3, \"__count_end\": 9.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 660.0, \"bin_maxbins_60_review_length_end\": 680.0, \"rating\": \"3star\"}, {\"__count\": 10, \"__count_end\": 34.0, \"__count_start\": 24.0, \"bin_maxbins_60_review_length\": 520.0, \"bin_maxbins_60_review_length_end\": 540.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 4.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 920.0, \"bin_maxbins_60_review_length_end\": 940.0, \"rating\": \"2star\"}, {\"__count\": 3, \"__count_end\": 3.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 700.0, \"bin_maxbins_60_review_length_end\": 720.0, \"rating\": \"5star\"}, {\"__count\": 29, \"__count_end\": 87.0, \"__count_start\": 58.0, \"bin_maxbins_60_review_length\": 380.0, \"bin_maxbins_60_review_length_end\": 400.0, \"rating\": \"3star\"}, {\"__count\": 21, \"__count_end\": 62.0, \"__count_start\": 41.0, \"bin_maxbins_60_review_length\": 440.0, \"bin_maxbins_60_review_length_end\": 460.0, \"rating\": \"2star\"}, {\"__count\": 28, \"__count_end\": 119.0, \"__count_start\": 91.0, \"bin_maxbins_60_review_length\": 400.0, \"bin_maxbins_60_review_length_end\": 420.0, \"rating\": \"2star\"}, {\"__count\": 4, \"__count_end\": 11.0, \"__count_start\": 7.0, \"bin_maxbins_60_review_length\": 700.0, \"bin_maxbins_60_review_length_end\": 720.0, \"rating\": \"2star\"}, {\"__count\": 17, \"__count_end\": 24.0, \"__count_start\": 7.0, \"bin_maxbins_60_review_length\": 440.0, \"bin_maxbins_60_review_length_end\": 460.0, \"rating\": \"4star\"}, {\"__count\": 69, \"__count_end\": 69.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 300.0, \"bin_maxbins_60_review_length_end\": 320.0, \"rating\": \"5star\"}, {\"__count\": 9, \"__count_end\": 25.0, \"__count_start\": 16.0, \"bin_maxbins_60_review_length\": 580.0, \"bin_maxbins_60_review_length_end\": 600.0, \"rating\": \"2star\"}, {\"__count\": 11, \"__count_end\": 30.0, \"__count_start\": 19.0, \"bin_maxbins_60_review_length\": 560.0, \"bin_maxbins_60_review_length_end\": 580.0, \"rating\": \"2star\"}, {\"__count\": 10, \"__count_end\": 35.0, \"__count_start\": 25.0, \"bin_maxbins_60_review_length\": 580.0, \"bin_maxbins_60_review_length_end\": 600.0, \"rating\": \"1star\"}, {\"__count\": 7, \"__count_end\": 7.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 500.0, \"bin_maxbins_60_review_length_end\": 520.0, \"rating\": \"5star\"}, {\"__count\": 4, \"__count_end\": 4.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 600.0, \"bin_maxbins_60_review_length_end\": 620.0, \"rating\": \"5star\"}, {\"__count\": 26, \"__count_end\": 26.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 380.0, \"bin_maxbins_60_review_length_end\": 400.0, \"rating\": \"5star\"}, {\"__count\": 6, \"__count_end\": 6.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 540.0, \"bin_maxbins_60_review_length_end\": 560.0, \"rating\": \"5star\"}, {\"__count\": 13, \"__count_end\": 13.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 480.0, \"bin_maxbins_60_review_length_end\": 500.0, \"rating\": \"5star\"}, {\"__count\": 4, \"__count_end\": 24.0, \"__count_start\": 20.0, \"bin_maxbins_60_review_length\": 540.0, \"bin_maxbins_60_review_length_end\": 560.0, \"rating\": \"2star\"}, {\"__count\": 25, \"__count_end\": 25.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 360.0, \"bin_maxbins_60_review_length_end\": 380.0, \"rating\": \"5star\"}, {\"__count\": 1, \"__count_end\": 2.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 840.0, \"bin_maxbins_60_review_length_end\": 860.0, \"rating\": \"4star\"}, {\"__count\": 7, \"__count_end\": 7.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 620.0, \"bin_maxbins_60_review_length_end\": 640.0, \"rating\": \"5star\"}, {\"__count\": 16, \"__count_end\": 29.0, \"__count_start\": 13.0, \"bin_maxbins_60_review_length\": 480.0, \"bin_maxbins_60_review_length_end\": 500.0, \"rating\": \"4star\"}, {\"__count\": 7, \"__count_end\": 23.0, \"__count_start\": 16.0, \"bin_maxbins_60_review_length\": 620.0, \"bin_maxbins_60_review_length_end\": 640.0, \"rating\": \"1star\"}, {\"__count\": 9, \"__count_end\": 15.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 540.0, \"bin_maxbins_60_review_length_end\": 560.0, \"rating\": \"4star\"}, {\"__count\": 17, \"__count_end\": 31.0, \"__count_start\": 14.0, \"bin_maxbins_60_review_length\": 460.0, \"bin_maxbins_60_review_length_end\": 480.0, \"rating\": \"4star\"}, {\"__count\": 2, \"__count_end\": 3.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 780.0, \"bin_maxbins_60_review_length_end\": 800.0, \"rating\": \"3star\"}, {\"__count\": 7, \"__count_end\": 21.0, \"__count_start\": 14.0, \"bin_maxbins_60_review_length\": 640.0, \"bin_maxbins_60_review_length_end\": 660.0, \"rating\": \"1star\"}, {\"__count\": 6, \"__count_end\": 11.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 560.0, \"bin_maxbins_60_review_length_end\": 580.0, \"rating\": \"4star\"}, {\"__count\": 6, \"__count_end\": 15.0, \"__count_start\": 9.0, \"bin_maxbins_60_review_length\": 660.0, \"bin_maxbins_60_review_length_end\": 680.0, \"rating\": \"2star\"}, {\"__count\": 7, \"__count_end\": 7.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 440.0, \"bin_maxbins_60_review_length_end\": 460.0, \"rating\": \"5star\"}, {\"__count\": 32, \"__count_end\": 58.0, \"__count_start\": 26.0, \"bin_maxbins_60_review_length\": 380.0, \"bin_maxbins_60_review_length_end\": 400.0, \"rating\": \"4star\"}, {\"__count\": 2, \"__count_end\": 4.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 800.0, \"bin_maxbins_60_review_length_end\": 820.0, \"rating\": \"3star\"}, {\"__count\": 6, \"__count_end\": 14.0, \"__count_start\": 8.0, \"bin_maxbins_60_review_length\": 640.0, \"bin_maxbins_60_review_length_end\": 660.0, \"rating\": \"2star\"}, {\"__count\": 4, \"__count_end\": 16.0, \"__count_start\": 12.0, \"bin_maxbins_60_review_length\": 620.0, \"bin_maxbins_60_review_length_end\": 640.0, \"rating\": \"2star\"}, {\"__count\": 3, \"__count_end\": 8.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 800.0, \"bin_maxbins_60_review_length_end\": 820.0, \"rating\": \"1star\"}, {\"__count\": 14, \"__count_end\": 76.0, \"__count_start\": 62.0, \"bin_maxbins_60_review_length\": 440.0, \"bin_maxbins_60_review_length_end\": 460.0, \"rating\": \"1star\"}, {\"__count\": 33, \"__count_end\": 152.0, \"__count_start\": 119.0, \"bin_maxbins_60_review_length\": 400.0, \"bin_maxbins_60_review_length_end\": 420.0, \"rating\": \"1star\"}, {\"__count\": 3, \"__count_end\": 7.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 920.0, \"bin_maxbins_60_review_length_end\": 940.0, \"rating\": \"1star\"}, {\"__count\": 1, \"__count_end\": 3.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 740.0, \"bin_maxbins_60_review_length_end\": 760.0, \"rating\": \"4star\"}, {\"__count\": 3, \"__count_end\": 7.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 700.0, \"bin_maxbins_60_review_length_end\": 720.0, \"rating\": \"3star\"}, {\"__count\": 8, \"__count_end\": 19.0, \"__count_start\": 11.0, \"bin_maxbins_60_review_length\": 560.0, \"bin_maxbins_60_review_length_end\": 580.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 12.0, \"__count_start\": 10.0, \"bin_maxbins_60_review_length\": 620.0, \"bin_maxbins_60_review_length_end\": 640.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 17.0, \"__count_start\": 15.0, \"bin_maxbins_60_review_length\": 660.0, \"bin_maxbins_60_review_length_end\": 680.0, \"rating\": \"1star\"}, {\"__count\": 7, \"__count_end\": 14.0, \"__count_start\": 7.0, \"bin_maxbins_60_review_length\": 500.0, \"bin_maxbins_60_review_length_end\": 520.0, \"rating\": \"4star\"}, {\"__count\": 7, \"__count_end\": 16.0, \"__count_start\": 9.0, \"bin_maxbins_60_review_length\": 580.0, \"bin_maxbins_60_review_length_end\": 600.0, \"rating\": \"3star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 820.0, \"bin_maxbins_60_review_length_end\": 840.0, \"rating\": \"5star\"}, {\"__count\": 6, \"__count_end\": 6.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 520.0, \"bin_maxbins_60_review_length_end\": 540.0, \"rating\": \"5star\"}, {\"__count\": 2, \"__count_end\": 4.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 900.0, \"bin_maxbins_60_review_length_end\": 920.0, \"rating\": \"2star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 860.0, \"bin_maxbins_60_review_length_end\": 880.0, \"rating\": \"2star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 800.0, \"bin_maxbins_60_review_length_end\": 820.0, \"rating\": \"5star\"}, {\"__count\": 2, \"__count_end\": 10.0, \"__count_start\": 8.0, \"bin_maxbins_60_review_length\": 680.0, \"bin_maxbins_60_review_length_end\": 700.0, \"rating\": \"2star\"}, {\"__count\": 5, \"__count_end\": 5.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 560.0, \"bin_maxbins_60_review_length_end\": 580.0, \"rating\": \"5star\"}, {\"__count\": 7, \"__count_end\": 17.0, \"__count_start\": 10.0, \"bin_maxbins_60_review_length\": 680.0, \"bin_maxbins_60_review_length_end\": 700.0, \"rating\": \"1star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 900.0, \"bin_maxbins_60_review_length_end\": 920.0, \"rating\": \"5star\"}, {\"__count\": 4, \"__count_end\": 7.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 820.0, \"bin_maxbins_60_review_length_end\": 840.0, \"rating\": \"1star\"}, {\"__count\": 4, \"__count_end\": 6.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 660.0, \"bin_maxbins_60_review_length_end\": 680.0, \"rating\": \"4star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 880.0, \"bin_maxbins_60_review_length_end\": 900.0, \"rating\": \"2star\"}, {\"__count\": 11, \"__count_end\": 40.0, \"__count_start\": 29.0, \"bin_maxbins_60_review_length\": 480.0, \"bin_maxbins_60_review_length_end\": 500.0, \"rating\": \"3star\"}, {\"__count\": 4, \"__count_end\": 5.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 720.0, \"bin_maxbins_60_review_length_end\": 740.0, \"rating\": \"3star\"}, {\"__count\": 7, \"__count_end\": 27.0, \"__count_start\": 20.0, \"bin_maxbins_60_review_length\": 600.0, \"bin_maxbins_60_review_length_end\": 620.0, \"rating\": \"1star\"}, {\"__count\": 4, \"__count_end\": 5.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 960.0, \"bin_maxbins_60_review_length_end\": 980.0, \"rating\": \"1star\"}, {\"__count\": 3, \"__count_end\": 3.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 920.0, \"bin_maxbins_60_review_length_end\": 940.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 5.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 780.0, \"bin_maxbins_60_review_length_end\": 800.0, \"rating\": \"1star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 680.0, \"bin_maxbins_60_review_length_end\": 700.0, \"rating\": \"5star\"}, {\"__count\": 1, \"__count_end\": 4.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 700.0, \"bin_maxbins_60_review_length_end\": 720.0, \"rating\": \"4star\"}, {\"__count\": 3, \"__count_end\": 8.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 840.0, \"bin_maxbins_60_review_length_end\": 860.0, \"rating\": \"1star\"}, {\"__count\": 3, \"__count_end\": 5.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 940.0, \"bin_maxbins_60_review_length_end\": 960.0, \"rating\": \"1star\"}, {\"__count\": 3, \"__count_end\": 9.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 600.0, \"bin_maxbins_60_review_length_end\": 620.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 6.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 640.0, \"bin_maxbins_60_review_length_end\": 660.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 960.0, \"bin_maxbins_60_review_length_end\": 980.0, \"rating\": \"5star\"}, {\"__count\": 3, \"__count_end\": 10.0, \"__count_start\": 7.0, \"bin_maxbins_60_review_length\": 620.0, \"bin_maxbins_60_review_length_end\": 640.0, \"rating\": \"4star\"}, {\"__count\": 3, \"__count_end\": 6.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 760.0, \"bin_maxbins_60_review_length_end\": 780.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 720.0, \"bin_maxbins_60_review_length_end\": 740.0, \"rating\": \"4star\"}, {\"__count\": 3, \"__count_end\": 5.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 680.0, \"bin_maxbins_60_review_length_end\": 700.0, \"rating\": \"4star\"}, {\"__count\": 2, \"__count_end\": 3.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 760.0, \"bin_maxbins_60_review_length_end\": 780.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 6.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 900.0, \"bin_maxbins_60_review_length_end\": 920.0, \"rating\": \"1star\"}, {\"__count\": 4, \"__count_end\": 15.0, \"__count_start\": 11.0, \"bin_maxbins_60_review_length\": 700.0, \"bin_maxbins_60_review_length_end\": 720.0, \"rating\": \"1star\"}, {\"__count\": 1, \"__count_end\": 4.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 780.0, \"bin_maxbins_60_review_length_end\": 800.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 6.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 740.0, \"bin_maxbins_60_review_length_end\": 760.0, \"rating\": \"1star\"}, {\"__count\": 2, \"__count_end\": 8.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 640.0, \"bin_maxbins_60_review_length_end\": 660.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 7.0, \"__count_start\": 5.0, \"bin_maxbins_60_review_length\": 720.0, \"bin_maxbins_60_review_length_end\": 740.0, \"rating\": \"2star\"}, {\"__count\": 4, \"__count_end\": 6.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 880.0, \"bin_maxbins_60_review_length_end\": 900.0, \"rating\": \"1star\"}, {\"__count\": 3, \"__count_end\": 5.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 840.0, \"bin_maxbins_60_review_length_end\": 860.0, \"rating\": \"2star\"}, {\"__count\": 2, \"__count_end\": 2.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 580.0, \"bin_maxbins_60_review_length_end\": 600.0, \"rating\": \"5star\"}, {\"__count\": 2, \"__count_end\": 4.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 860.0, \"bin_maxbins_60_review_length_end\": 880.0, \"rating\": \"1star\"}, {\"__count\": 1, \"__count_end\": 5.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 740.0, \"bin_maxbins_60_review_length_end\": 760.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 2.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 900.0, \"bin_maxbins_60_review_length_end\": 920.0, \"rating\": \"3star\"}, {\"__count\": 2, \"__count_end\": 6.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 600.0, \"bin_maxbins_60_review_length_end\": 620.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 2.0, \"__count_start\": 1.0, \"bin_maxbins_60_review_length\": 820.0, \"bin_maxbins_60_review_length_end\": 840.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 3.0, \"__count_start\": 2.0, \"bin_maxbins_60_review_length\": 820.0, \"bin_maxbins_60_review_length_end\": 840.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 5.0, \"__count_start\": 4.0, \"bin_maxbins_60_review_length\": 800.0, \"bin_maxbins_60_review_length_end\": 820.0, \"rating\": \"2star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 840.0, \"bin_maxbins_60_review_length_end\": 860.0, \"rating\": \"5star\"}, {\"__count\": 1, \"__count_end\": 4.0, \"__count_start\": 3.0, \"bin_maxbins_60_review_length\": 740.0, \"bin_maxbins_60_review_length_end\": 760.0, \"rating\": \"3star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 760.0, \"bin_maxbins_60_review_length_end\": 780.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 1.0, \"__count_start\": 0.0, \"bin_maxbins_60_review_length\": 780.0, \"bin_maxbins_60_review_length_end\": 800.0, \"rating\": \"4star\"}, {\"__count\": 1, \"__count_end\": 7.0, \"__count_start\": 6.0, \"bin_maxbins_60_review_length\": 760.0, \"bin_maxbins_60_review_length_end\": 780.0, \"rating\": \"1star\"}]}, {\"name\": \"data_1_color_domain_rating_0\", \"values\": [{\"rating\": \"4star\"}, {\"rating\": \"1star\"}, {\"rating\": \"2star\"}, {\"rating\": \"5star\"}, {\"rating\": \"3star\"}]}, {\"name\": \"data_2_color_domain_rating_1\", \"values\": [{\"rating\": \"4star\"}, {\"rating\": \"1star\"}, {\"rating\": \"2star\"}, {\"rating\": \"5star\"}, {\"rating\": \"3star\"}]}, {\"name\": \"data_0_concat_0_x_domain_rating\", \"values\": [{\"rating\": \"4star\", \"sort_field\": 3.0}, {\"rating\": \"1star\", \"sort_field\": 0.0}, {\"rating\": \"2star\", \"sort_field\": 1.0}, {\"rating\": \"5star\", \"sort_field\": 4.0}, {\"rating\": \"3star\", \"sort_field\": 2.0}]}, {\"name\": \"data_1_concat_0_y_domain___count\", \"values\": [{\"min\": 5531, \"max\": 5651}]}], \"signals\": [{\"name\": \"concat_1_bin_maxbins_60_review_length_bins\", \"value\": {\"fields\": [\"review_length\"], \"fname\": \"bin_review_length\", \"start\": 0.0, \"step\": 20.0, \"stop\": 980.0}}, {\"name\": \"concat_0_width\", \"value\": 300}, {\"name\": \"concat_1_width\", \"value\": 400}], \"marks\": [{\"type\": \"group\", \"name\": \"concat_0_group\", \"encode\": {\"update\": {\"height\": {\"signal\": \"height\"}, \"width\": {\"signal\": \"concat_0_width\"}}}, \"marks\": [{\"type\": \"rect\", \"name\": \"concat_0_marks\", \"from\": {\"data\": \"data_1\"}, \"encode\": {\"update\": {\"fill\": {\"field\": \"rating\", \"scale\": \"color\"}, \"x\": {\"field\": \"rating\", \"scale\": \"concat_0_x\"}, \"y\": {\"field\": \"__count\", \"scale\": \"concat_0_y\"}, \"tooltip\": {\"signal\": \"{\\\"rating\\\": isValid(datum[\\\"rating\\\"]) ? datum[\\\"rating\\\"] : \\\"\\\"+datum[\\\"rating\\\"], \\\"Count of Records\\\": format(datum[\\\"__count\\\"], \\\"\\\")}\"}, \"width\": {\"signal\": \"max(0.25, bandwidth('concat_0_x'))\"}, \"y2\": {\"value\": 0, \"scale\": \"concat_0_y\"}}}, \"style\": [\"bar\"]}], \"axes\": [{\"scale\": \"concat_0_y\", \"aria\": false, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"orient\": \"left\", \"gridScale\": \"concat_0_x\", \"maxExtent\": 0, \"domain\": false, \"zindex\": 0, \"labels\": false, \"ticks\": false, \"grid\": true, \"minExtent\": 0}, {\"scale\": \"concat_0_x\", \"labelAlign\": \"right\", \"title\": \"Rating\", \"labelAngle\": 270, \"labelBaseline\": \"middle\", \"zindex\": 0, \"grid\": false, \"orient\": \"bottom\"}, {\"scale\": \"concat_0_y\", \"orient\": \"left\", \"title\": \"Number of Reviews\", \"labelOverlap\": true, \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"zindex\": 0, \"grid\": false}], \"title\": {\"text\": \"Distribution of Ratings\", \"frame\": \"group\"}, \"style\": \"cell\"}, {\"type\": \"group\", \"name\": \"concat_1_group\", \"encode\": {\"update\": {\"width\": {\"signal\": \"concat_1_width\"}, \"height\": {\"signal\": \"height\"}}}, \"marks\": [{\"type\": \"rect\", \"name\": \"concat_1_marks\", \"from\": {\"data\": \"data_2\"}, \"encode\": {\"update\": {\"x2\": {\"field\": \"bin_maxbins_60_review_length\", \"scale\": \"concat_1_x\", \"offset\": {\"signal\": \"0.5 + (abs(scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length_end\\\"]) - scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length\\\"])) < 0.25 ? -0.5 * (0.25 - (abs(scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length_end\\\"]) - scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length\\\"])))) : 0.5)\"}}, \"y2\": {\"field\": \"__count_start\", \"scale\": \"concat_1_y\"}, \"y\": {\"field\": \"__count_end\", \"scale\": \"concat_1_y\"}, \"fill\": {\"field\": \"rating\", \"scale\": \"color\"}, \"tooltip\": {\"signal\": \"{\\\"Count of Records\\\": format(datum[\\\"__count\\\"], \\\"\\\")}\"}, \"x\": {\"field\": \"bin_maxbins_60_review_length_end\", \"scale\": \"concat_1_x\", \"offset\": {\"signal\": \"0.5 + (abs(scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length_end\\\"]) - scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length\\\"])) < 0.25 ? 0.5 * (0.25 - (abs(scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length_end\\\"]) - scale(\\\"concat_1_x\\\", datum[\\\"bin_maxbins_60_review_length\\\"])))) : -0.5)\"}}}}, \"style\": [\"bar\"]}], \"axes\": [{\"scale\": \"concat_1_y\", \"zindex\": 0, \"ticks\": false, \"gridScale\": \"concat_1_x\", \"maxExtent\": 0, \"aria\": false, \"grid\": true, \"orient\": \"left\", \"tickCount\": {\"signal\": \"ceil(height/40)\"}, \"labels\": false, \"domain\": false, \"minExtent\": 0}, {\"scale\": \"concat_1_x\", \"labelFlush\": true, \"title\": \"Word Count per Review\", \"tickCount\": {\"signal\": \"ceil(concat_1_width/10)\"}, \"orient\": \"bottom\", \"zindex\": 0, \"labelOverlap\": true, \"grid\": false}, {\"scale\": \"concat_1_y\", \"zindex\": 0, \"grid\": false, \"orient\": \"left\", \"title\": \"Frequency\", \"labelOverlap\": true, \"tickCount\": {\"signal\": \"ceil(height/40)\"}}], \"title\": {\"text\": \"Distribution of Review Lengths\", \"frame\": \"group\"}, \"style\": \"cell\"}], \"scales\": [{\"name\": \"color\", \"type\": \"ordinal\", \"domain\": {\"fields\": [{\"data\": \"data_1_color_domain_rating_0\", \"field\": \"rating\"}, {\"data\": \"data_2_color_domain_rating_1\", \"field\": \"rating\"}], \"sort\": true}, \"range\": \"category\"}, {\"name\": \"concat_0_x\", \"type\": \"band\", \"domain\": {\"data\": \"data_0_concat_0_x_domain_rating\", \"field\": \"rating\", \"sort\": {\"op\": \"max\", \"field\": \"sort_field\"}}, \"range\": [0, {\"signal\": \"concat_0_width\"}], \"paddingOuter\": 0.05, \"paddingInner\": 0.1}, {\"name\": \"concat_0_y\", \"type\": \"linear\", \"domain\": [{\"signal\": \"(data(\\\"data_1_concat_0_y_domain___count\\\")[0] || {}).min\"}, {\"signal\": \"(data(\\\"data_1_concat_0_y_domain___count\\\")[0] || {}).max\"}], \"range\": [{\"signal\": \"height\"}, 0], \"zero\": true, \"nice\": true}, {\"name\": \"concat_1_x\", \"type\": \"linear\", \"domain\": {\"signal\": \"[concat_1_bin_maxbins_60_review_length_bins.start, concat_1_bin_maxbins_60_review_length_bins.stop]\"}, \"range\": [0, {\"signal\": \"concat_1_width\"}], \"bins\": {\"signal\": \"concat_1_bin_maxbins_60_review_length_bins\"}, \"zero\": false}, {\"name\": \"concat_1_y\", \"type\": \"linear\", \"domain\": {\"data\": \"data_2\", \"fields\": [\"__count_start\", \"__count_end\"]}, \"range\": [{\"signal\": \"height\"}, 0], \"zero\": true, \"nice\": true}], \"background\": \"white\", \"layout\": {\"padding\": 20, \"bounds\": \"full\", \"align\": \"each\"}, \"padding\": 5, \"height\": 300}, {\"mode\": \"vega\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.HConcatChart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "### EDA training set\n",
        "import copy\n",
        "\n",
        "train_eda = train_set.copy()\n",
        "train_eda['review_length'] = train_set['content'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Rating Plot\n",
        "ratings = alt.Chart(train_eda).mark_bar().encode(\n",
        "    x = alt.X('rating').sort(['1star', '2star', '3star', '4star', '5star']).title('Rating'),\n",
        "    y = alt.Y('count()').title('Number of Reviews'),\n",
        "    color = alt.Color('rating').legend(None),\n",
        "    tooltip = ['rating', 'count()']\n",
        ").properties(\n",
        "    title = 'Distribution of Ratings',\n",
        "    width = 300,\n",
        "    height = 300\n",
        ")\n",
        "\n",
        "# Length Plot\n",
        "length = alt.Chart(train_eda).mark_bar().encode(\n",
        "    x=alt.X('review_length', bin=alt.Bin(maxbins=60), title='Word Count per Review'),\n",
        "    y=alt.Y('count()', title='Frequency'),\n",
        "    color = alt.Color('rating').legend(None),\n",
        "    tooltip=['count()']\n",
        ").properties(\n",
        "    title='Distribution of Review Lengths',\n",
        "    width=400,\n",
        "    height=300\n",
        ")\n",
        "\n",
        "final_chart = ratings | length\n",
        "final_chart\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyaYq4wm5HDc"
      },
      "source": [
        "From EDA, we know that the classes are balanced, and it seems like all the ratings are in proportion to the length of the comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKzPLELl5HDc"
      },
      "source": [
        "# Baseline\n",
        "\n",
        "For baseline, I will use tfidf vectorizer and logistic regression, it is fast and provide a decent standard baseline score for my further models.\n",
        "\n",
        "The baseline Macro F1 Score = 59.36%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mBWEB25D5HDc"
      },
      "outputs": [],
      "source": [
        "### baseline\n",
        "\n",
        "## import\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.pipeline import make_pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oi25SuCN5HDc",
        "outputId": "22f87c5d-7817-4b3c-8e21-8b86cb4ffb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28000 3500\n"
          ]
        }
      ],
      "source": [
        "## data split\n",
        "\n",
        "X_train = train_set['content']\n",
        "y_train = train_set['rating']\n",
        "\n",
        "X_dev = dev_set['content']\n",
        "y_dev = dev_set['rating']\n",
        "\n",
        "print(len(X_train), len(X_dev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NpEHWnAL5HDd"
      },
      "outputs": [],
      "source": [
        "## Pipeline\n",
        "baseline = make_pipeline(\n",
        "    TfidfVectorizer(\n",
        "        stop_words = 'english',\n",
        "        ngram_range = (1, 2)\n",
        "    ),\n",
        "\n",
        "    LogisticRegression(\n",
        "        solver = 'liblinear',\n",
        "        C = 1.0\n",
        "    )\n",
        ")\n",
        "\n",
        "# baseline train & pred\n",
        "baseline.fit(X_train, y_train)\n",
        "y_pred_baseline = baseline.predict(X_dev)\n",
        "\n",
        "# calculate macro f1\n",
        "f1_baseline = f1_score(y_dev, y_pred_baseline, average = 'macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DGkG7KVO5HDd",
        "outputId": "ccd41535-b36f-475f-c0e5-91043b05bf8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Macro F1 score of baseline with tfidf and logistic is: 0.5936185962662195\n"
          ]
        }
      ],
      "source": [
        "print(f'The Macro F1 score of baseline with tfidf and logistic is: {f1_baseline}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MCpNacL5HDd"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OZ9cenRd5HDd"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_set.rating)\n",
        "\n",
        "train_y = label_encoder.transform(train_set.rating)\n",
        "dev_y = label_encoder.transform(dev_set.rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6tYHczFf5HDd"
      },
      "outputs": [],
      "source": [
        "# Vocabulary\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for sentence in train_set.content:\n",
        "    tokens = sentence.lower().split()\n",
        "    vocabulary.update(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NHjwm9py5HDd"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "## Create a w2i\n",
        "def BuildWord2i(contents):\n",
        "    counter = Counter()\n",
        "\n",
        "    for content in contents:\n",
        "        tokens = str(content).lower().split()\n",
        "        counter.update(tokens)\n",
        "\n",
        "    word2i = {}\n",
        "    word2i['<PAD>'] = 0\n",
        "    word2i['<UNK>'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for word, count in counter.items():\n",
        "        word2i[word] = idx\n",
        "        idx += 1\n",
        "\n",
        "    return word2i\n",
        "\n",
        "## get w2i\n",
        "w2i = BuildWord2i(train_set['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "V9DLjuUg5HDd",
        "outputId": "7f9ee7e3-cfaa-41dd-def1-f937c8974b91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([106121, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "## Get the embedding matrix from spacy\n",
        "def BuildEmbeddingMatrix(word2i, vocab_size, emb_dim = 300):\n",
        "    weights_matrix = np.random.normal(scale = 0.6, size = (vocab_size, emb_dim))\n",
        "\n",
        "    weights_matrix[w2i['<PAD>']] = np.zeros((emb_dim,))\n",
        "    count = 0\n",
        "    for word, i in w2i.items():\n",
        "        if word in nlp.vocab and nlp.vocab[word].has_vector:\n",
        "            weights_matrix[i] = nlp.vocab[word].vector\n",
        "            found_count += 1\n",
        "    return torch.tensor(weights_matrix, dtype = torch.float32)\n",
        "\n",
        "## create embedding weights from spacy\n",
        "EMBEDDING_DIM = 300\n",
        "embedding_weights = BuildEmbeddingMatrix(w2i, len(w2i), EMBEDDING_DIM)\n",
        "embedding_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yWasmVaB5HDd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def create_data_loader(df, y, w2i, batch_size=32, shuffle=True):\n",
        "    pad_token_id = w2i.get('<PAD>', 0)\n",
        "    unk_token_id = w2i.get('<UNK>', 0)\n",
        "\n",
        "    def text_to_indices(text):\n",
        "        tokens = text.split()\n",
        "        return [w2i.get(token, unk_token_id) for token in tokens]\n",
        "\n",
        "    indices_list = [torch.tensor(text_to_indices(text)) for text in df['content']]\n",
        "    padded_inputs = pad_sequence(indices_list, batch_first=True, padding_value=pad_token_id)\n",
        "\n",
        "    labels = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(padded_inputs, labels)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return loader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = create_data_loader(train_set, train_y, w2i, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = create_data_loader(dev_set, dev_y,  w2i, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ts2GxEGP5HDd"
      },
      "outputs": [],
      "source": [
        "# CBOW originated from lab 3\n",
        "HIDDEN_SIZE = 100\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, weights_matrix, num_classes, dropout_prob, padding_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(weights_matrix, padding_idx = padding_idx)\n",
        "        self.embedding_dim = weights_matrix.shape[1]\n",
        "\n",
        "        self.linear1 = nn.Linear(self.embedding_dim, HIDDEN_SIZE)\n",
        "        self.dropout = nn.Dropout(p = dropout_prob)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(HIDDEN_SIZE, num_classes)\n",
        "\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "            non_pad_mask = (x != self.padding_idx)\n",
        "            lengths = non_pad_mask.sum(dim=1).float().clamp(min=1).unsqueeze(1)\n",
        "\n",
        "            embedded = self.embedding(x)\n",
        "            x = torch.sum(embedded, dim=1) / lengths\n",
        "\n",
        "            x = self.linear1(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.linear2(x)\n",
        "            return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "op265a9a5HDd"
      },
      "outputs": [],
      "source": [
        "pad_idx = w2i['<PAD>']\n",
        "\n",
        "model = CBOW(\n",
        "    weights_matrix = embedding_weights,\n",
        "    num_classes = 5,\n",
        "    dropout_prob = 0.5,\n",
        "    padding_idx = pad_idx\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PqBsw9t85HDd"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n_xOiUSc5HDd",
        "outputId": "cc7f651d-bb52-42c5-fa56-c64a3e7f07f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Train Loss: 1.4408, Val Loss: 1.3469, Val Acc: 41.74%\n",
            "Epoch [2/3], Train Loss: 1.3760, Val Loss: 1.3383, Val Acc: 42.23%\n",
            "Epoch [3/3], Train Loss: 1.3639, Val Loss: 1.3268, Val Acc: 42.54%\n",
            "Finish processing\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 3\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dev_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(dev_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], '\n",
        "          f'Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "print(\"Finish processing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiV46zXr5HDd"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XLBlvQJq5HDd"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, weights_matrix, num_classes, hidden_size = 256, num_layers = 1, dropout_prob = 0.5, padding_idx = 0):\n",
        "        super().__init__()\n",
        "\n",
        "        weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            weights_tensor,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "        self.emb_dim = weights_matrix.shape[1]\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout_prob if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        lengths = (x != self.padding_idx).sum(dim=1).cpu()\n",
        "\n",
        "        embeds = self.embedding(x) # [Batch, Seq, Emb]\n",
        "        packed_input = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
        "\n",
        "        cat_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "\n",
        "        output = self.dropout(cat_hidden)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jVSBQk5e5HDe",
        "outputId": "dbd4483a-cda9-44d5-e99a-3fe39058a2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2886251409.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n"
          ]
        }
      ],
      "source": [
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = w2i['<PAD>']\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "model = LSTM(\n",
        "    weights_matrix=embedding_weights,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout_prob=DROPOUT,\n",
        "    padding_idx=PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lw7d_OdD5HDe",
        "outputId": "8e4b1a21-cd18-4a00-b8a1-69b5608a267a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save best model (F1: 0.4521)\n",
            "Epoch [1], Train Loss: 1.4031, Val Loss: 1.2327, Val F1 (Macro): 0.4521\n",
            "Val Loss (1/3)\n",
            "Epoch [2], Train Loss: 1.2334, Val Loss: 1.2392, Val F1 (Macro): 0.4346\n",
            "Save best model (F1: 0.4686)\n",
            "Epoch [3], Train Loss: 1.1421, Val Loss: 1.1874, Val F1 (Macro): 0.4686\n",
            "Save best model (F1: 0.5289)\n",
            "Epoch [4], Train Loss: 1.0361, Val Loss: 1.0636, Val F1 (Macro): 0.5289\n",
            "Save best model (F1: 0.5310)\n",
            "Epoch [5], Train Loss: 0.9297, Val Loss: 1.0446, Val F1 (Macro): 0.5310\n",
            "Val Loss (1/3)\n",
            "Epoch [6], Train Loss: 0.8368, Val Loss: 1.0540, Val F1 (Macro): 0.5501\n",
            "Val Loss (2/3)\n",
            "Epoch [7], Train Loss: 0.7459, Val Loss: 1.0646, Val F1 (Macro): 0.5570\n",
            "Val Loss (3/3)\n",
            "Early stop triggers\n",
            "Finish training\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train_loss = float('inf')\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "trigger_times = 0\n",
        "epoch = 0\n",
        "THRESHOLD = 0.01\n",
        "MAX_EPOCHS = 100\n",
        "\n",
        "while train_loss > THRESHOLD and epoch < MAX_EPOCHS:\n",
        "    epoch += 1\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_loss = avg_train_loss\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dev_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(dev_loader)\n",
        "\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"Save best model (F1: {val_f1:.4f})\")\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"Val Loss ({trigger_times}/{patience})\")\n",
        "\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stop triggers\")\n",
        "            break\n",
        "\n",
        "    print(f'Epoch [{epoch}], '\n",
        "          f'Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Val F1 (Macro): {val_f1:.4f}')\n",
        "\n",
        "print(\"Finish training\")\n",
        "model.load_state_dict(torch.load('best_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F43P4AbV5HDe",
        "outputId": "d03763b3-95d8-4a1b-db0b-abf50752c52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2886251409.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n"
          ]
        }
      ],
      "source": [
        "# Hyper parameter tunning\n",
        "\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = w2i['<PAD>']\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "model_tuned = LSTM(\n",
        "    weights_matrix=embedding_weights,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout_prob=DROPOUT,\n",
        "    padding_idx=PAD_IDX\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_tuned.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7eoFwcvz5HDe",
        "outputId": "6970f897-d1ab-4c52-ba96-845cd1a60c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save best model_tuned (F1: 0.3612)\n",
            "Epoch [1], Train Loss: 1.4040, Val Loss: 1.2835, Val F1 (Macro): 0.3612\n",
            "Save best model_tuned (F1: 0.4822)\n",
            "Epoch [2], Train Loss: 1.2019, Val Loss: 1.1576, Val F1 (Macro): 0.4822\n",
            "Save best model_tuned (F1: 0.5128)\n",
            "Epoch [3], Train Loss: 1.0639, Val Loss: 1.0784, Val F1 (Macro): 0.5128\n",
            "Save best model_tuned (F1: 0.5407)\n",
            "Epoch [4], Train Loss: 0.9518, Val Loss: 1.0268, Val F1 (Macro): 0.5407\n",
            "Val Loss (1/3)\n",
            "Epoch [5], Train Loss: 0.8555, Val Loss: 1.0524, Val F1 (Macro): 0.5452\n",
            "Val Loss (2/3)\n",
            "Epoch [6], Train Loss: 0.7450, Val Loss: 1.0955, Val F1 (Macro): 0.5398\n",
            "Val Loss (3/3)\n",
            "Early stop triggers\n",
            "Finish training\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train_loss = float('inf')\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "trigger_times = 0\n",
        "epoch = 0\n",
        "THRESHOLD = 0.01\n",
        "MAX_EPOCHS = 100\n",
        "\n",
        "while train_loss > THRESHOLD and epoch < MAX_EPOCHS:\n",
        "    epoch += 1\n",
        "    model_tuned.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_tuned(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_loss = avg_train_loss\n",
        "\n",
        "    model_tuned.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dev_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model_tuned(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(dev_loader)\n",
        "\n",
        "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model_tuned.state_dict(), 'best_model_tuned.pth')\n",
        "        print(f\"Save best model_tuned (F1: {val_f1:.4f})\")\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"Val Loss ({trigger_times}/{patience})\")\n",
        "\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stop triggers\")\n",
        "            break\n",
        "\n",
        "    print(f'Epoch [{epoch}], '\n",
        "          f'Train Loss: {avg_train_loss:.4f}, '\n",
        "          f'Val Loss: {avg_val_loss:.4f}, '\n",
        "          f'Val F1 (Macro): {val_f1:.4f}')\n",
        "\n",
        "print(\"Finish training\")\n",
        "model_tuned.load_state_dict(torch.load('best_model_tuned.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipGTlEB5HDf"
      },
      "source": [
        "# predict on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eW4efH6v5HDf"
      },
      "outputs": [],
      "source": [
        "test_set['rating'] = '1star'\n",
        "test_loader = create_data_loader(test_set, w2i = w2i, y = label_encoder.transform(test_set.rating), batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BXgmIcMr5HDf",
        "outputId": "32bc1e6c-a7b2-4ea9-c549-5036339c789d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star2',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star0',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star4',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star2',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star4',\n",
              " 'star2',\n",
              " 'star1',\n",
              " 'star3',\n",
              " 'star0',\n",
              " 'star1',\n",
              " 'star3',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def get_preds(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            preds = predicted.cpu().numpy()\n",
        "\n",
        "            all_preds.extend(['star' + str(x) for x in preds])\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "test_preds = get_preds(model_tuned, test_loader)\n",
        "test_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sZpszxSu5HDf"
      },
      "outputs": [],
      "source": [
        "out_prediction('Tianhao', 'Cao', test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "filenames = ['best_model.pth', 'best_model_tuned.pth', 'Tianhao_Cao_PRED.txt']\n",
        "\n",
        "for fname in filenames:\n",
        "    print(f\"downloading {fname}...\")\n",
        "    files.download(fname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "7NjV_oZU8qPA",
        "outputId": "e1643cfe-3f9d-4e0b-8e8a-127e8ce2edbd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading best_model.pth...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1faa1012-0a1d-44f7-95e3-a5f244e35385\", \"best_model.pth\", 130696571)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading best_model_tuned.pth...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5c35d151-ab34-4897-a4db-ab3ebb333572\", \"best_model_tuned.pth\", 144546999)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading Tianhao_Cao_PRED.txt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f62b97a6-859a-4e85-9dc3-29c40cc9191a\", \"Tianhao_Cao_PRED.txt\", 21006)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVZ8YCXK8uUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}